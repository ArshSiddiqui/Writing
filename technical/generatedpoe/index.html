<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Edgar Allen Poe-try#the day broke it
so strong and a willingness
to let it atone
I have been interested in language modeling for a while, and I have done many projects in that realm in the past. Firstly, langauge modeling is the use of a technique to determine the probablility of a sentence occuring with the words in it in that order. It is frequently used to create computer algorithms that will create intelligible sentences based upon some text corpus.">
<meta name="theme-color" content="#FFFFFF">
<meta name="color-scheme" content="light dark"><meta property="og:title" content="Generated Poe-try" />
<meta property="og:description" content="Edgar Allen Poe-try#the day broke it
so strong and a willingness
to let it atone
I have been interested in language modeling for a while, and I have done many projects in that realm in the past. Firstly, langauge modeling is the use of a technique to determine the probablility of a sentence occuring with the words in it in that order. It is frequently used to create computer algorithms that will create intelligible sentences based upon some text corpus." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://arshsiddiqui.github.io/writing/technical/generatedpoe/" /><meta property="article:section" content="technical" />


<title>Generated Poe-try | Arsh Siddiqui</title>
<link rel="manifest" href="/writing/manifest.json">
<link rel="icon" href="/writing/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/writing/book.min.b3488045b94246c08cb956e8a82d99944a7cbd581bf976a700e281bbb73b1a07.css" integrity="sha256-s0iARblCRsCMuVboqC2ZlEp8vVgb&#43;XanAOKBu7c7Ggc=" crossorigin="anonymous">
  <script defer src="/writing/flexsearch.min.js"></script>
  <script defer src="/writing/en.search.min.14c1566bf00552efc72dc423d0c071b93df70fbb036efc51bda163802ae688cb.js" integrity="sha256-FMFWa/AFUu/HLcQj0MBxuT33D7sDbvxRvaFjgCrmiMs=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/writing/"><span>Arsh Siddiqui</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>



  



  
    
  



<ul class="book-languages">
  <li>
    <input type="checkbox" id="languages" class="toggle" />
    <label for="languages" class="flex justify-between">
      <a role="button" class="flex align-center">
        <img src="/writing/svg/translate.svg" class="book-icon" alt="Languages" />
        English
      </a>
    </label>

    <ul>
      
      <li>
        <a href="https://arshsiddiqui.github.io/writing/fr/">
          French
        </a>
      </li>
      
    </ul>
  </li>
</ul>











  <ul>
<li><a href="/writing/technical/">Technical</a></li>
<li><a href="/writing/poetry/">Poetry</a></li>
<li><a href="/writing/bookreviews/">Book Reviews</a></li>
<li><a href="/writing/oldwriting/">Old Writing</a></li>
<li><a href="/writing/others/">Others</a></li>
</ul>










</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/writing/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Generated Poe-try</strong>

  <label for="toc-control">
    
    <img src="/writing/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents"></nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="edgar-allen-poe-try">
  Edgar Allen Poe-try
  <a class="anchor" href="#edgar-allen-poe-try">#</a>
</h1>
<blockquote>
<p>the day broke it<br>
so strong and a willingness<br>
to let it atone</p>
</blockquote>
<p>I have been interested in language modeling for a while, and I have done many projects in that realm in the
past. Firstly, langauge modeling is the use of a technique to determine the probablility of a sentence occuring
with the words in it in that order. It is frequently used to create computer algorithms that will create
intelligible sentences based upon some text corpus.</p>
<p>I first began working with language models with the bag of words method, which simply categorizes all of the
words in some given text into it based upon their appearance in the text. We can then take this categorization
and output a randomized string of words based upon some randomness and word commonality. Using this method
created sentences like this:</p>
<blockquote>
<p>too, no best than about clearly in know of in made of the the side one the of and the are customer From spots
having found majority, from was been I-as Touch-me-Not a double The having armed dropped rank but which mer_.
well week-once</p>
</blockquote>
<p>Yeah, it isn&rsquo;t very good, but it was a start. I then moved on to n-gram models. This effectively takes n - 1 words
and uses them to predict the next word. So say we have a bi-gram (2-gram), this means that we can take a sentence
and break it upon into groups of two words:</p>
<blockquote>
<p>There is a dog named Allen.</p>
</blockquote>
<blockquote>
<p><strong>[ (There, is), (is, a), (a, dog), (dog, named), (named, Allen) ]</strong></p>
</blockquote>
<p>Using the occurences of bundled words together, when we give an algorithm a word, it can determine what comes
next based upon the previous word (what normally comes up based upon our text corpus). I attemped to create an
algorithm that worked based upon this method a while ago, though I believe that I created it incorrectly or
did it poorly, because the final sentences again weren&rsquo;t that great:</p>
<blockquote>
<p>no psyche within none within hair of like i brilliant i of thee in shore vogue land shore the native thy seas
too whole grandeur agate thee stand inner long its here scorn ere scorn was byron which it safely remarkable
estimate teens can none grandeur withering lamp withering</p>
</blockquote>
<p>I did, much later get back into language modeling. For a class project, a friend and I decided to create a trigram
model, which is effectively the same as a bigram model but with n = 3. So that previous sentence would be broken
down like this:</p>
<blockquote>
<p>There is a dog named Allen.</p>
</blockquote>
<blockquote>
<p><strong>[ (There, is, a), (is, a, dog), (a, dog, named), (dog, named, Allen) ]</strong></p>
</blockquote>
<p>So instead of looking at the previous word to determine the next word, we would look at the previous two words to
determine the next, and so on so forth. For this project (and all the previous), we used a library called <code>nltk</code>.
This effectively just makes created all those bigrams and trigams much easier than they would otherwise be, and
they make creating the models much easier. Our final product ended up being much better than the previous
examples, as the following sentences exhibits (note that we used the novel &ldquo;The Count of Monte Cristo&rdquo; for our
text corpus here, whereas for the previous projects I had used a text corpus of Edgar Allen Poe stories):</p>
<blockquote>
<p>and then drew himself up with happiness, looking for all that you had conveyed a packet for Marshal Bertrand.</p>
</blockquote>
<blockquote>
<p>the night of my heart. Is it really your intention to make me captain of the city.</p>
</blockquote>
<p>Yeah, those sentences aren&rsquo;t that bad, I think.</p>
<p>More recently, I wanted to try to use a similar trigram algorithm on Edgar Allen Poe writing to create haikus. This
would, of course, not only require text generation, but also need to have a way to convert the sentences that my
text generation algorithm would create into haikus. Before I go on too far into that, let&rsquo;s first look at the
trigram model itself. Firstly, we need to open the text file and create the model:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span>: defaultdict(<span style="color:#66d9ef">lambda</span>: <span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>file <span style="color:#f92672">=</span> open(<span style="color:#e6db74">&#34;EAPStories.txt&#34;</span>, <span style="color:#e6db74">&#39;r&#39;</span>)
</span></span></code></pre></div><p>The second line just opens the text file to be read only, while the first line creates the base for the model
itself. Effectively what it is doing is creating a dictionary within a dictionary. This allows us to pair two
words together to determine what the next possible words could be.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> file:
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">for</span> w1, w2, w3 <span style="color:#f92672">in</span> trigrams(line<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split(), pad_right<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, pad_left<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>		model[(w1, w2)][w3] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		total_occurence <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> occurence <span style="color:#f92672">in</span> model[<span style="color:#e6db74">&#39;the&#39;</span>, <span style="color:#e6db74">&#39;day&#39;</span>]<span style="color:#f92672">.</span>values():
</span></span><span style="display:flex;"><span>		total_occurence <span style="color:#f92672">+=</span> occurence
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> w1w2 <span style="color:#f92672">in</span> model:
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> w3 <span style="color:#f92672">in</span> model[w1w2]:
</span></span><span style="display:flex;"><span>		model[w1w2][w3] <span style="color:#f92672">=</span> model[w1w2][w3] <span style="color:#f92672">/</span> total_occurence
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dict(model[<span style="color:#e6db74">&#39;and&#39;</span>, <span style="color:#e6db74">&#39;then&#39;</span>])
</span></span></code></pre></div><pre tabindex="0"><code>{&#39;went&#39;: 0.015151515151515152,
&#39;obtain&#39;: 0.015151515151515152,
&#39;i&#39;: 0.12121212121212122,
&#39;returned,&#39;: 0.015151515151515152,
&#39;from&#39;: 0.030303030303030304,
&#39;caught&#39;: 0.015151515151515152,
&#39;the&#39;: 0.16666666666666666,
&#39;universal&#39;: 0.015151515151515152,
&#39;felt&#39;: 0.015151515151515152,
None: 0.12121212121212122,
...
&#39;producing&#39;: 0.015151515151515152}
</code></pre><p>This code firstly goes through every line in the text file and splits the words up in very similar fashion
to that of the &ldquo;There is a dog named Allen&rdquo; example I&rsquo;d previously shown. The rest of the code takes every
single possible third word to the first two words in the trigram, and determines the probability and word
will come next based upon the total number of possibilities. This can be seen in the output, which is showing
all of the possible concluding words to the two words &ldquo;and&rdquo; and &ldquo;then&rdquo;. Each word is correlated to a
probability of its occurence. The &ldquo;None&rdquo; which can be seen as well just dictates the end of the sentence.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_sentence</span>(word1, word2):
</span></span><span style="display:flex;"><span>	sentence <span style="color:#f92672">=</span> [word1, word2]
</span></span><span style="display:flex;"><span>	sentence_complete <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>	total_words <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>	num_words <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> sentence_complete:
</span></span><span style="display:flex;"><span>		min_words <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>random()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> model[tuple(sentence[num_words <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span>:])]<span style="color:#f92672">.</span>keys():
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			total_words <span style="color:#f92672">+=</span> model[tuple(sentence[num_words <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span>:])][word]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">if</span> total_words <span style="color:#f92672">&gt;=</span> min_words:
</span></span><span style="display:flex;"><span>				sentence<span style="color:#f92672">.</span>append(word)
</span></span><span style="display:flex;"><span>				num_words <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>				<span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">if</span> sentence[num_words <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span>:] <span style="color:#f92672">==</span> [<span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>]:
</span></span><span style="display:flex;"><span>			sentence_complete <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	sentence_str <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> sentence:
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">if</span> word <span style="color:#f92672">!=</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>			sentence_str <span style="color:#f92672">=</span> sentence_str <span style="color:#f92672">+</span> word <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; &#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	print(<span style="color:#e6db74">&#34;create_sentence:&#34;</span>, sentence_str)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> sentence
</span></span></code></pre></div><p>While analyzing our data visually is indeed very useful, it is also helpful to analyze our data
quantitatively. Here, we can also use some Python to calculate the returns between our start and end date
and we can also use this to determine an average return across each year (or month if we wanted to do that
as well).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>days <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>years <span style="color:#f92672">=</span> days <span style="color:#f92672">/</span> <span style="color:#ae81ff">365</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>final_returns <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span><span style="color:#f92672">*</span>(np<span style="color:#f92672">.</span>exp(log_returns<span style="color:#f92672">.</span>cumsum()) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Return from start date to end date is: &#39;</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{:.2f}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format(final_returns) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;%&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>final_returns_per_year <span style="color:#f92672">=</span> final_returns <span style="color:#f92672">/</span> years
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Average yearly return: &#39;</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{:.2f}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format(final_returns_per_year) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;%&#39;</span>)
</span></span></code></pre></div><pre tabindex="0"><code>Return from start date to end date is: 178.52%
Average yearly return: 65.16%
</code></pre><p>This is finally the code that creates our sentences. What we are effectively doing is setting a random
probability that we want each of our next words to achieve at a threshold. Then we continuously add
these words to a list that will become our sentence. The sentence ends once we hit the word &ldquo;None&rdquo;
(which refers to no word) twice.</p>
<p>This is similar to what we had done for the Monte Cristo trigam model, but I have changed some of the
methods involved for simplicities sake.</p>
<p>The rest of the program will generate a sentence using the above function, checks how many syllables
are in each word, and then attempt to create the three haiku lines with those words in their proper
order. If it can&rsquo;t, it scraps that sentence and generates a new one. This keeps going until a haiku
is created. You can see one of the haikus that the algorithm generated at the top of this page.</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents"></nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












